
# ğŸ§  Medical Data ETL Pipeline

A simple **ETL (Extract, Transform, Load)** pipeline built with **Python + Pandas**, designed to clean and process a medical dataset in a fully containerized environment using **Docker**, **Docker Compose**, and automated **CI/CD** using **GitHub Actions**.

---

## ğŸš€ Features

* ğŸ“¥ **Extract** â€” Reads raw medical data from a CSV file  
* ğŸ”„ **Transform** â€” Cleans missing values and normalizes column names  
* ğŸ“¤ **Load** â€” Saves the cleaned dataset to a new CSV file  
* ğŸ³ **Dockerized** â€” Easily run the entire pipeline using Docker  
* ğŸ§© **Compose Ready** â€” Includes `docker-compose.yml` for seamless orchestration  
* âš™ï¸ **CI/CD Integration** â€” Automatically builds and pushes Docker images to Docker Hub on each push to `main` branch  

---

## ğŸ“‚ Project Structure

```

â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ ci-cd.yml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ Medicaldataset.csv
â”‚   â””â”€â”€ CleanedMedicalData.csv (generated)
â”œâ”€â”€ main.py
â””â”€â”€ README.md

````

---

## âš™ï¸ How It Works

1. **Extract:** Reads the raw dataset from `/data/Medicaldataset.csv`.  
2. **Transform:**  
   * Drops rows with missing values  
   * Normalizes column names (lowercase + underscores)  
3. **Load:** Saves the cleaned data as `/data/CleanedMedicalData.csv`.  
4. **CI/CD:**  
   * On every push to the `main` branch, GitHub Actions automatically:  
     - Builds the Docker image  
     - Pushes it to Docker Hub (`dhiraj918106/simple-data-pipeline:latest`)  

---

## ğŸ³ Run with Docker

### 1ï¸âƒ£ Build the image

```bash
docker build -t medical-etl .
````

### 2ï¸âƒ£ Run the container

```bash
docker run -v $(pwd)/data:/data medical-etl
```

### 3ï¸âƒ£ Or use Docker Compose

```bash
docker-compose up
```

---

## âš™ï¸ CI/CD Pipeline

This project includes a **GitHub Actions workflow** (`.github/workflows/ci-cd.yml`) that:

1. Checks out the repository
2. Logs in to Docker Hub using encrypted secrets (`DOCKERHUB_USERNAME`, `DOCKERHUB_TOKEN`)
3. Builds and pushes the Docker image automatically

### ğŸ§© Trigger

The pipeline runs **automatically** on every push to the `main` branch.

---

## âœ… Output

After running, youâ€™ll find your cleaned dataset here:

```
/data/CleanedMedicalData.csv
```

And your latest Docker image will be available on Docker Hub as:

```
dhiraj918106/simple-data-pipeline:latest
```

---

## ğŸ’¡ Example Logs

```
Data extraction completed  
Data transformation completed  
Data loading completed  
Data pipeline completed successfully  
```

---

